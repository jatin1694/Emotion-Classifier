{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T16:41:32.659294Z",
     "start_time": "2019-04-14T16:41:29.330913Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "import traceback\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten, Dense\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T16:41:34.003256Z",
     "start_time": "2019-04-14T16:41:33.992440Z"
    }
   },
   "outputs": [],
   "source": [
    "def list_files(startpath, return_neutral = False):\n",
    "\n",
    "    all_files = []\n",
    "    neutral_files = []\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        files = sorted(files)\n",
    "        if files and files[-1][-4] == '.' and files[0][-4] == '.':\n",
    "            all_files.append(root + '/' + files[-1])\n",
    "            neutral_files.append(root + '/' + files[0])\n",
    "\n",
    "    if return_neutral:\n",
    "        return sorted(all_files), sorted(neutral_files)\n",
    "    else:\n",
    "        return sorted(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T16:41:34.959492Z",
     "start_time": "2019-04-14T16:41:34.952842Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_matching(X, y):\n",
    "    final_X = []\n",
    "    final_y = []\n",
    "    i, j = 0, 0\n",
    "    while i < len(X):\n",
    "        X_split = X[i].split('/')\n",
    "        y_split = y[j].split('/')\n",
    "        if X_split[3] == y_split[3] and X_split[4] == y_split[4]:\n",
    "            final_X.append(X[i])\n",
    "            final_y.append(y[j])\n",
    "            j+=1\n",
    "        i+=1\n",
    "    return final_X, final_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T16:41:35.495921Z",
     "start_time": "2019-04-14T16:41:35.492029Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_Y(y):\n",
    "    res_y = []\n",
    "    for file in y:\n",
    "        with open(file) as f:\n",
    "            for line in f.readlines():\n",
    "                res_y.append(float(line))\n",
    "    return np.array(res_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T16:41:35.998066Z",
     "start_time": "2019-04-14T16:41:35.992573Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_X(X):\n",
    "    res_X = []\n",
    "    try:\n",
    "        for file in X:\n",
    "            img = cv2.imread(file,1) # reads image as color\n",
    "            img, bboxes = face_reduction(img)\n",
    "            res_X.extend(img)\n",
    "    except Exception as e:\n",
    "        print(\"Exception in X\")\n",
    "        print(X)\n",
    "        print(e)\n",
    "    return np.array(res_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T16:41:37.037995Z",
     "start_time": "2019-04-14T16:41:37.027442Z"
    }
   },
   "outputs": [],
   "source": [
    "def detectFaceOpenCVDnn(net, frame):\n",
    "    result = []\n",
    "    frameOpencvDnn = frame.copy()\n",
    "    frameHeight = frameOpencvDnn.shape[0]\n",
    "    frameWidth = frameOpencvDnn.shape[1]\n",
    "    blob = cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], False, False)\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    bboxes = []\n",
    "    conf_threshold = 0.7\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > conf_threshold:\n",
    "            x1 = int(detections[0, 0, i, 3] * frameWidth)\n",
    "            y1 = int(detections[0, 0, i, 4] * frameHeight)\n",
    "            x2 = int(detections[0, 0, i, 5] * frameWidth)\n",
    "            y2 = int(detections[0, 0, i, 6] * frameHeight)\n",
    "            bboxes.append([x1, y1, x2, y2])\n",
    "            if x1 > frameWidth or x1 < 0 or x2 > frameWidth or x2 < 0 or y1 < 0 or y1 > frameHeight or y2 < 0 or y2 > frameHeight:\n",
    "                continue\n",
    "            else:\n",
    "                grayOpenDnn = gray = cv2.cvtColor(frameOpencvDnn, cv2.COLOR_BGR2GRAY)\n",
    "                croppedOpenDnn = cv2.resize(gray[y1:y2,x1:x2], (200,200)) \n",
    "                result.append(croppedOpenDnn)\n",
    "#                 cv2.rectangle(frameOpencvDnn, (x1, y1), (x2, y2), (0, 255, 0), int(round(frameHeight/150)), 8)\n",
    "    return result, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T16:41:37.712885Z",
     "start_time": "2019-04-14T16:41:37.708434Z"
    }
   },
   "outputs": [],
   "source": [
    "def face_reduction(image):\n",
    "    # OpenCV DNN supports 2 networks.\n",
    "    # 1. FP16 version of the original caffe implementation ( 5.4 MB )\n",
    "    # 2. 8 bit Quantized version using Tensorflow h( 2.7 MB )\n",
    "#     print(\"printing image\")\n",
    "#     print(image)\n",
    "    DNN = \"TF\"\n",
    "    if DNN == \"CAFFE\":\n",
    "        modelFile = \"models/res10_300x300_ssd_iter_140000_fp16.caffemodel\"\n",
    "        configFile = \"models/deploy.prototxt\"\n",
    "        net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
    "    else:\n",
    "        modelFile = \"models/opencv_face_detector_uint8.pb\"\n",
    "        configFile = \"models/opencv_face_detector.pbtxt\"\n",
    "        net = cv2.dnn.readNetFromTensorflow(modelFile, configFile)\n",
    "\n",
    "    conf_threshold = 0.7\n",
    "    outOpencvDnn, bboxes = detectFaceOpenCVDnn(net,image)\n",
    "    return outOpencvDnn, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T16:41:38.972496Z",
     "start_time": "2019-04-14T16:41:38.962707Z"
    }
   },
   "outputs": [],
   "source": [
    "def face_landmarks(frame, rects):\n",
    "    # initialize dlib's face detector (HOG-based) and then create\n",
    "    # the facial landmark predictor\n",
    "    p = \"shape_predictor_68_face_landmarks.dat\"\n",
    "    predictor = dlib.shape_predictor(p)\n",
    "    # loop over the face detections\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        # determine the facial landmarks for the face region, then\n",
    "        # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "        # array\n",
    "        shape = predictor(frame, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "#         print(shape, shape.shape)\n",
    "        # loop over the (x, y)-coordinates for the facial landmarks\n",
    "        # and draw them on the image\n",
    "        for (x, y) in shape:\n",
    "            cv2.circle(image, (x, y), 2, (0, 255, 0), -1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T22:53:33.010589Z",
     "start_time": "2019-04-09T22:51:42.115922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(906, 200, 200)\n",
      "(906, 200, 200, 1)\n",
      "the total number of unique training points are: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0., 1., 2., 3., 4., 5., 6., 7.]),\n",
       " array([583,  44,  17,  59,  24,  69,  27,  83]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'neutral', 1: 'anger', 2: 'contempt', 3: 'disgust', 4: 'fear', 5: 'happy', 6: 'sadness', 7: 'surprise'}\n"
     ]
    }
   ],
   "source": [
    "y_translation = {0:\"neutral\", 1:\"anger\", 2:\"contempt\", 3:\"disgust\", 4:\"fear\", 5:\"happy\", 6:\"sadness\", 7:\"surprise\"}\n",
    "X, X_neutral = list_files('../Data/cohn-kanade-images/', return_neutral= True)\n",
    "y = list_files('../Data/Emotion/')\n",
    "X, y = find_matching(X, y)\n",
    "y = read_Y(y)\n",
    "X = read_X(X)\n",
    "X_neutral = read_X(X_neutral)\n",
    "\n",
    "X_train= np.vstack((X, X_neutral))\n",
    "print(X_train.shape)\n",
    "X_train = X_train.reshape((X_train.shape[0],X_train.shape[1],X_train.shape[2],1))\n",
    "print(X_train.shape)\n",
    "total_y = np.hstack((y, np.zeros(X_neutral.shape[0]))).reshape(-1,1)\n",
    "\n",
    "oht = OneHotEncoder(categories='auto', sparse=False)\n",
    "y_train = oht.fit_transform(total_y)\n",
    "print(\"the total number of unique training points are: \")\n",
    "np.unique(total_y, return_counts=True)\n",
    "print(y_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KERAS NEURAL NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T22:53:33.750423Z",
     "start_time": "2019-04-09T22:53:33.012550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/visheshhemnani/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/visheshhemnani/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = 200\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size = (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(96, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(Conv2D(32, kernel_size=(3,3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(8, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T22:53:33.798765Z",
     "start_time": "2019-04-09T22:53:33.752379Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T23:02:34.242075Z",
     "start_time": "2019-04-09T22:53:33.800982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/visheshhemnani/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "906/906 [==============================] - 39s 43ms/step - loss: 1.1044 - acc: 0.6865\n",
      "Epoch 2/20\n",
      "906/906 [==============================] - 31s 35ms/step - loss: 0.5895 - acc: 0.8344\n",
      "Epoch 3/20\n",
      "906/906 [==============================] - 31s 34ms/step - loss: 0.3664 - acc: 0.8896\n",
      "Epoch 4/20\n",
      "906/906 [==============================] - 31s 34ms/step - loss: 0.2798 - acc: 0.9128\n",
      "Epoch 5/20\n",
      "906/906 [==============================] - 31s 34ms/step - loss: 0.1575 - acc: 0.9547\n",
      "Epoch 6/20\n",
      "906/906 [==============================] - 31s 34ms/step - loss: 0.0957 - acc: 0.9713\n",
      "Epoch 7/20\n",
      "906/906 [==============================] - 31s 35ms/step - loss: 0.0536 - acc: 0.9834\n",
      "Epoch 8/20\n",
      "906/906 [==============================] - 25s 28ms/step - loss: 0.0337 - acc: 0.9923\n",
      "Epoch 9/20\n",
      "906/906 [==============================] - 24s 27ms/step - loss: 0.0240 - acc: 0.9945\n",
      "Epoch 10/20\n",
      "906/906 [==============================] - 24s 27ms/step - loss: 0.0166 - acc: 0.9989\n",
      "Epoch 11/20\n",
      "906/906 [==============================] - 24s 26ms/step - loss: 0.0126 - acc: 1.0000\n",
      "Epoch 12/20\n",
      "906/906 [==============================] - 24s 26ms/step - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 13/20\n",
      "906/906 [==============================] - 24s 27ms/step - loss: 0.0050 - acc: 1.0000\n",
      "Epoch 14/20\n",
      "906/906 [==============================] - 24s 26ms/step - loss: 0.1549 - acc: 0.9536\n",
      "Epoch 15/20\n",
      "906/906 [==============================] - 24s 27ms/step - loss: 0.1528 - acc: 0.9437\n",
      "Epoch 16/20\n",
      "906/906 [==============================] - 24s 26ms/step - loss: 0.0637 - acc: 0.9812\n",
      "Epoch 17/20\n",
      "906/906 [==============================] - 24s 27ms/step - loss: 0.0726 - acc: 0.9790\n",
      "Epoch 18/20\n",
      "906/906 [==============================] - 24s 27ms/step - loss: 0.0796 - acc: 0.9768\n",
      "Epoch 19/20\n",
      "906/906 [==============================] - 24s 27ms/step - loss: 0.0389 - acc: 0.9890\n",
      "Epoch 20/20\n",
      "906/906 [==============================] - 24s 26ms/step - loss: 0.0225 - acc: 0.9945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a45f79400>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=50, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T16:41:44.144025Z",
     "start_time": "2019-04-14T16:41:44.135918Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_emotions(videoFaces, frame, bboxes):\n",
    "#     emotion = []\n",
    "    outputFrame = frame.copy()\n",
    "    predictions = model.predict(videoFaces)\n",
    "#     print(\"YOUR EMOTION IS -  \")\n",
    "#     print(y_translation[np.argmax(predictions[0])])\n",
    "    \n",
    "#     for pred in predictions:\n",
    "#         emotion.append(pred) \n",
    "    for i in range(len(bboxes)):\n",
    "        emotion = y_translation[np.argmax(predictions[i])]\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(outputFrame, emotion, (bboxes[i][0] -1 ,bboxes[i][1] - 1), font, 1, (0,255,0), 2, cv2.LINE_AA)\n",
    "        cv2.rectangle(outputFrame, (bboxes[i][0], bboxes[i][1]), (bboxes[i][2], bboxes[i][3]), (0, 255, 0), int(round(outputFrame.shape[0]/150)), 8)\n",
    "    return outputFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T16:41:44.826319Z",
     "start_time": "2019-04-14T16:41:44.816597Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_webcam_feed():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    hasFrame, frame = cap.read()\n",
    "    \n",
    "    frame_count = 0\n",
    "    tt_opencvDnn = 0\n",
    "    while(1):\n",
    "        try:\n",
    "            hasFrame, frame = cap.read()\n",
    "            if not hasFrame:\n",
    "                break\n",
    "            frame_count += 1\n",
    "            outputFrame = frame\n",
    "            print(frame.shape)\n",
    "            #Gives a list of gray-scale images in webcam feed\n",
    "            videoFaces, bboxes = face_reduction(frame)\n",
    "            \n",
    "            videoFaces = np.array(videoFaces)\n",
    "            if videoFaces.shape[0] != 0:\n",
    "                videoFaces = videoFaces.reshape((videoFaces.shape[0],videoFaces.shape[1],videoFaces.shape[2],1))\n",
    "                face_landmarks(videoFaces[0], bboxes)\n",
    "                break\n",
    "                outputFrame = display_emotions(videoFaces, frame, bboxes)\n",
    "                \n",
    "            cv2.imshow(\"frame\", outputFrame)\n",
    "            k = cv2.waitKey(10)\n",
    "            if k == 27:\n",
    "                break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"Exception is \")\n",
    "            print(e)\n",
    "            traceback.print_exc()\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T16:41:48.853843Z",
     "start_time": "2019-04-14T16:41:45.609024Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "Exception is \n",
      "Unable to cast Python instance to C++ type (compile in debug mode for details)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-92df19723776>\", line 21, in get_webcam_feed\n",
      "    face_landmarks(videoFaces[0], bboxes)\n",
      "  File \"<ipython-input-8-263a1d49d036>\", line 11, in face_landmarks\n",
      "    shape = predictor(frame, rect)\n",
      "RuntimeError: Unable to cast Python instance to C++ type (compile in debug mode for details)\n"
     ]
    }
   ],
   "source": [
    "get_webcam_feed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T16:26:10.189254Z",
     "start_time": "2019-04-14T16:26:08.974493Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True [[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    hasFrame, frame = cap.read()\n",
    "    if hasFrame:\n",
    "        break\n",
    "frame\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
