{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T20:47:24.124374Z",
     "start_time": "2019-04-24T20:47:24.112565Z"
    }
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten, Dense\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T17:54:22.922827Z",
     "start_time": "2019-04-24T17:54:10.308261Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "LOLYEhG-2U-b"
   },
   "outputs": [],
   "source": [
    "y_translation = {\n",
    "    0: 'Male',\n",
    "    1: 'Female',\n",
    "}\n",
    "X_train, X_test, y_train, y_test = read_UTK(UTK_path)\n",
    "classes = 2\n",
    "inputShape = (200, 200, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Neural Network Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T20:31:22.515489Z",
     "start_time": "2019-04-24T20:31:22.506057Z"
    }
   },
   "outputs": [],
   "source": [
    "class Keras_NN:\n",
    "    def __init__(self,X_train, Y_train):\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        \n",
    "    def train_model(self):\n",
    "        IMG_SIZE = 48\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, kernel_size = (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "        model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "        model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(7, activation = 'softmax'))\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        model.fit(self.X_train, self.Y_train, batch_size=len(self.X_train)//20, epochs=20, verbose=1)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def load_model(self):\n",
    "        model = load_model('keras_on_fer')\n",
    "        return model\n",
    "    \n",
    "    def save_model(self, model, name):\n",
    "        model.save(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebCam Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T20:48:46.149286Z",
     "start_time": "2019-04-24T20:48:46.140973Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_emotions(model_emotion,model_gender, emotion_faces,gender_faces, frame, bboxes):\n",
    "    '''\n",
    "    Fetches the emotions by using model prediction and adds \n",
    "    them to the output frame\n",
    "    '''\n",
    "    emotion_translation = {\n",
    "        0: 'Angry',\n",
    "        1: 'Disgust',\n",
    "        2: 'Fear',\n",
    "        3: 'Happy',\n",
    "        4: 'Sad',\n",
    "        5: 'Surprise',\n",
    "        6: 'Neutral'\n",
    "    }\n",
    "    \n",
    "    gender_translation = {\n",
    "        0: ' Man',\n",
    "        1: ' Woman',\n",
    "    }\n",
    "    outputFrame = frame.copy()\n",
    "    emotions = model_emotion.predict(emotion_faces)\n",
    "    gender = model_gender.predict(gender_faces)\n",
    "    for i in range(len(bboxes)):\n",
    "        emotion = emotion_translation[np.argmax(emotions[i])]\n",
    "        gender = gender_translation[np.argmax(gender[i])]\n",
    "        result = emotion + gender\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(outputFrame, result,\n",
    "                    (bboxes[i][0] - 1, bboxes[i][1] - 1), font, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.rectangle(outputFrame, (bboxes[i][0], bboxes[i][1]), (\n",
    "            bboxes[i][2], bboxes[i][3]), (0, 255, 0), int(round(outputFrame.shape[0]/150)), 8)\n",
    "    return outputFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T20:48:46.585156Z",
     "start_time": "2019-04-24T20:48:46.574841Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_webcam_feed(model_emotion, model_gender):\n",
    "    '''\n",
    "    Initializes webcam video for live classification\n",
    "    Extracts faces from the frame\n",
    "    Outputs the frame with predicted emotions to a window \n",
    "    '''\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    hasFrame, frame = cap.read()\n",
    "    # TODO : Handle cases for face going outside the frame\n",
    "    frame_count = 0\n",
    "    tt_opencvDnn = 0\n",
    "    while(1):\n",
    "        try:\n",
    "            hasFrame, frame = cap.read()\n",
    "            if not hasFrame:\n",
    "                break\n",
    "            frame_count += 1\n",
    "            outputFrame = frame\n",
    "\n",
    "            # Gives a list of gray-scale images in webcam feed\n",
    "            emotion_faces, bboxes = face_reduction(frame, (48, 48))\n",
    "            gender_faces, bboxes = face_reduction(frame, (200, 200))\n",
    "            emotion_faces = np.array(emotion_faces)\n",
    "            gender_faces = np.array(gender_faces)\n",
    "            if emotion_faces.shape[0] != 0:\n",
    "                #                 print(videoFaces.shape)\n",
    "                emotion_faces = emotion_faces.reshape(\n",
    "                    (emotion_faces.shape[0], emotion_faces.shape[1], emotion_faces.shape[2], 1))\n",
    "                gender_faces = gender_faces.reshape(\n",
    "                    (gender_faces.shape[0], gender_faces.shape[1], gender_faces.shape[2], 1))\n",
    "                outputFrame = display_emotions(\n",
    "                    model_emotion, model_gender, emotion_faces, gender_faces, frame, bboxes)\n",
    "            cv2.imshow(\"frame\", outputFrame)\n",
    "            k = cv2.waitKey(10)\n",
    "\n",
    "            # quits window when escape key is pressed\n",
    "            if k == 27:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(\"Exception is \")\n",
    "            print(e)\n",
    "            traceback.print_tb(e.__traceback__)\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T20:48:47.488414Z",
     "start_time": "2019-04-24T20:48:47.480107Z"
    }
   },
   "outputs": [],
   "source": [
    "def detectFaceOpenCVDnn(net, frame,image_size):\n",
    "    result = []\n",
    "    frameOpencvDnn = frame.copy()\n",
    "    frameHeight = frameOpencvDnn.shape[0]\n",
    "    frameWidth = frameOpencvDnn.shape[1]\n",
    "    blob = cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], False, False)\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    bboxes = []\n",
    "    conf_threshold = 0.7\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > conf_threshold:\n",
    "            x1 = int(detections[0, 0, i, 3] * frameWidth)\n",
    "            y1 = int(detections[0, 0, i, 4] * frameHeight)\n",
    "            x2 = int(detections[0, 0, i, 5] * frameWidth)\n",
    "            y2 = int(detections[0, 0, i, 6] * frameHeight)\n",
    "            bboxes.append([x1, y1, x2, y2])\n",
    "            if x1 > frameWidth or x1 < 0 or x2 > frameWidth or x2 < 0 or y1 < 0 or y1 > frameHeight or y2 < 0 or y2 > frameHeight:\n",
    "                continue\n",
    "            else:\n",
    "                grayOpenDnn = gray = cv2.cvtColor(frameOpencvDnn, cv2.COLOR_BGR2GRAY)\n",
    "                croppedOpenDnn = cv2.resize(gray[y1:y2,x1:x2], image_size) \n",
    "                result.append(croppedOpenDnn)\n",
    "    return result, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T20:48:47.888376Z",
     "start_time": "2019-04-24T20:48:47.883682Z"
    }
   },
   "outputs": [],
   "source": [
    "def face_reduction(image, image_size):\n",
    "    # OpenCV DNN supports 2 networks.\n",
    "    # 1. FP16 version of the original caffe implementation ( 5.4 MB )\n",
    "    # 2. 8 bit Quantized version using Tensorflow h( 2.7 MB )\n",
    "#     print(\"printing image\")\n",
    "#     print(image)\n",
    "    DNN = \"TF\"\n",
    "    if DNN == \"CAFFE\":\n",
    "        modelFile = \"models/res10_300x300_ssd_iter_140000_fp16.caffemodel\"\n",
    "        configFile = \"models/deploy.prototxt\"\n",
    "        net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
    "    else:\n",
    "        modelFile = \"models/opencv_face_detector_uint8.pb\"\n",
    "        configFile = \"models/opencv_face_detector.pbtxt\"\n",
    "        net = cv2.dnn.readNetFromTensorflow(modelFile, configFile)\n",
    "\n",
    "    conf_threshold = 0.7\n",
    "    outOpencvDnn, bboxes = detectFaceOpenCVDnn(net,image,image_size)\n",
    "    return outOpencvDnn, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T20:48:48.337020Z",
     "start_time": "2019-04-24T20:48:48.333554Z"
    }
   },
   "outputs": [],
   "source": [
    "model_NN = Keras_NN([],[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T20:48:58.722068Z",
     "start_time": "2019-04-24T20:48:48.756520Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/visheshhemnani/anaconda3/envs/ml_proj/lib/python3.6/site-packages/keras/engine/saving.py:327: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "emotion_model = model_NN.load_model()\n",
    "gender_model = load_model('gender_classifier_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-24T20:48:49.503Z"
    }
   },
   "outputs": [],
   "source": [
    "get_webcam_feed(emotion_model,gender_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
